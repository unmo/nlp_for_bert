{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session2-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxsLrTdPJlVXiajzH1r2Tw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unmo/nlp_for_bert/blob/main/memo/session2/session2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onKJQPn-QWCE"
      },
      "source": [
        "# シンプルなBertの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IA6nsRgQZ5Y",
        "outputId": "dc355147-cd5a-46eb-a695-ee824ec7d16d"
      },
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install urllib==1.25.11\n",
        "!pip install pytorch_transformers==1.2.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79809 sha256=178dc168d81a6aafead55861a0e9de5303ae94f7aa2392de0c6953681653748e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement urllib==1.25.11 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for urllib==1.25.11\u001b[0m\n",
            "Collecting pytorch_transformers==1.2.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.2.0) (1.10.0+cu111)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.22-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 39.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.2.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.2.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.2.0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.2.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers==1.2.0) (3.10.0.2)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.22\n",
            "  Downloading botocore-1.23.22-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 28.6 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.22->boto3->pytorch_transformers==1.2.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.22->boto3->pytorch_transformers==1.2.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.2.0) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers==1.2.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers==1.2.0) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.20.22 botocore-1.23.22 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s5Lkq0ZQ6xP"
      },
      "source": [
        "## 文章の一部の予測\n",
        "文章における一部の単語をmaskし、それをBERTのモデルを使って予測する(MaskedLM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGHWhq93QmbR",
        "outputId": "9a9401a4-c573-4e48-a0a4-c20998fac27b"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertForMaskedLM\n",
        "from pytorch_transformers import BertTokenizer\n",
        "\n",
        "\n",
        "text = \"[CLS] I played baseball with my friends at school yesterday [SEP]\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2614711.44B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', 'baseball', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVPTSbLR7_1"
      },
      "source": [
        "文章の一部をMASKする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osRMLLNPRyMz",
        "outputId": "3b9768a3-3703-43ed-be71-2aba6d50f232"
      },
      "source": [
        "msk_idx = 3\n",
        "words[msk_idx] = \"[MASK]\"  # 単語を[MASK]に置き換える\n",
        "print(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', '[MASK]', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSnFjzPhSKrO"
      },
      "source": [
        "単語を対応するインデックスに変換する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurSIUU1SNwy",
        "outputId": "0724e55e-f3eb-4dec-cbf6-33977d450167"
      },
      "source": [
        "word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "print(word_tensor)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 1045, 2209,  103, 2007, 2026, 2814, 2012, 2082, 7483,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcUvqhSmSlMc"
      },
      "source": [
        "BERTのモデルを使って予測を行う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGw5bGt7SyiI",
        "outputId": "c16aeb8c-d105-4b00-8930-d59b9b549aac"
      },
      "source": [
        "msk_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "msk_model.cuda()\n",
        "msk_model.eval()  # 評価モード\n",
        "\n",
        "x = word_tensor.cuda()\n",
        "y = msk_model(x)\n",
        "result = y[0]\n",
        "print(result.size())  # tensorの場合、sizeでshapeが見れる\n",
        "\n",
        "_, max_ids = torch.topk(result[0][msk_idx], k=10)  # 最も大きい5つの値\n",
        "result_words = tokenizer.convert_ids_to_tokens(max_ids.tolist())  # インデックスを単語に変換\n",
        "\n",
        "print(result_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 83305.06B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 34908635.96B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 11, 30522])\n",
            "['basketball', 'football', 'soccer', 'baseball', 'tennis', 'chess', 'golf', 'guitar', 'pool', 'softball']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A3ZhgDWaYB"
      },
      "source": [
        "## 文章が連続しているかどうかの判定\n",
        "\n",
        "BERTのモデルを使って、2つの文章が連続しているかどうかの判定を行う(Next Sentence Prediction)\n",
        "show_continuityでは、2つの文章の連続性を判定し、表示する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDdqYXXoW086"
      },
      "source": [
        "from pytorch_transformers import BertForNextSentencePrediction\n",
        "\n",
        "def show_continuity(text, seg_ids):\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "    ids_tensor = torch.tensor([ids])\n",
        "    seg_tensor = torch.tensor([seg_ids])\n",
        "\n",
        "    print(ids_tensor)\n",
        "    print(seg_tensor)\n",
        "\n",
        "    x = ids_tensor.cuda()\n",
        "    s = seg_tensor.cuda()\n",
        "\n",
        "    nsp_model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
        "    nsp_model.cuda()\n",
        "    nsp_model.eval()\n",
        "\n",
        "    y = nsp_model(x, s)\n",
        "    print(y)\n",
        "    result = torch.softmax(y[0], dim=1)\n",
        "    print(result)\n",
        "    # print(nsp_model)\n",
        "    print(f\"連続確率： {result[0][0].item()*100}\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[CLS] What is soccer ? [SEP] It is a game of shoot the boal [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2e4DxA4mio6",
        "outputId": "cc10e345-0cd6-4ae8-9c34-7208e5bebd6d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 2054, 2003, 4715, 1029,  102, 2009, 2003, 1037, 2208, 1997, 5607,\n",
            "         1996, 8945, 2389,  102]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "(tensor([[ 5.5447, -4.9307]], device='cuda:0', grad_fn=<AddmmBackward0>),)\n",
            "tensor([[9.9997e-01, 2.8221e-05]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "連続確率： 99.99717473983765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[CLS] What is soccer ? [SEP] This is made with flour and milk [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQwD7N-aty6K",
        "outputId": "f1da96f3-be72-4346-8091-811bef7be79f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  2054,  2003,  4715,  1029,   102,  2023,  2003,  2081,  2007,\n",
            "         13724,  1998,  6501,   102]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "(tensor([[-4.1400,  7.1774]], device='cuda:0', grad_fn=<AddmmBackward0>),)\n",
            "tensor([[1.2160e-05, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "連続確率： 0.0012159755897300784\n"
          ]
        }
      ]
    }
  ]
}